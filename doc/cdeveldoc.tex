\documentclass[oneside,12pt]{memoir}
\def\mychaplineone{Developer notes for}
\def\mychaplinetwo{CLIPC/ESGF integration}
%\def\mychaplinethree{And Some More...}
%\def\mychapbyline{Some crap that came to my mind!}
%\def\myauthone{Prashanth Dwarakanath}
%\def\myauthtwo{Hamish Struthers}
%\def\myauththree{Torgny Fax\'en}
\def\mypress{NSC and SMHI, Link\"oping Sweden}
\usepackage{chengi}
\def\phname{SODA API{ }}
\def\vernum{November 12, 2014}
\setcounter{tocdepth}{2}
\thispagestyle{empty}
\titleGM
\chapterstyle{BlueBox}
\pagestyle{mystyle}
\begin{document}
\frontmatter
\hypertarget{mytocmarker}
\tableofcontents
\mainmatter
\setcounter{secnumdepth}{2}
\chapter{Solution Design}
\section{Overview}
This document describes work to be done under the \textbf{CLIPC FP7 EU project}, in the \textbf{Climate Data Access Work Package (WP5)}, listed under task \textbf{T5.4 (Dynamic Tape-archive extraction and post-processing)}. The deliverable of this exercise is to be able to develop a generic system that would make it possible to  make data present on different kinds of offline tape systems, available for searching and retrieval, to ESGF users. This work targets the publication of data from the EURO4M project present on teh MARS system at SMHI-LUE as a demonstrator, as specified by task \textbf{T5.4}. This system should be customizable for use by other data centers with other offline systems, such as UK's Met Office. Broadly, this involves the following steps.
\begin{enumerate}
\item Publishing the offline data on ESGF so that it can be searched and discovered using the ESGF web-frontend.
\item Integrate the ESGF web-frontend to the new service in a manner such that
\begin{itemize}
\item When a user requests for offline data, it is determined whether the data has been cached from a previous query; if cached, the user is immediately provided a normal ESGF wget script with the download URLs.
\item If the data is not present in the cache, initiating a data fetch request to the backend tape system, to retrieve the requested data while providing the user a script that could be executed to query the status of the operation, and to download the finalwget script, when the data is available on disk.
\end{itemize}
\item Since the space available on disk is most likely limited, an intelligent algorithm to expire datasets on disk. The system should be aware of the status of all datasets present on the tape system which have been published on ESGF, whether they are currently available on disk or not.
\end{enumerate}
\section{Code Modules}
Initial design was brainstormed during the IS-ENES2 GA meet and further modified over subsequent meetings. This would consist of
\begin{enumerate}
\item An ESGF specific \textbf{System for Offline Data Access} (SODA) REST API, designed to be used by the web-frontend and/or user-executable scripts, to pass on the user requests to a plugin that interacts with a backend data archival system. This system would be generic and would allow for the use of plugins for systems other than MARS and would be the base part of the system.
\item A plugin, to be able to do ESGF to backend system  query conversions and perform other interactions with the backend system. The plugin is the customizable component and can be written to work with different backend systems. It would be up to the plugin developer to decide how to interact with the actual underlying system, i.e. whether to use a binary client, or another API, to do the actual communication with the backend system. 
In our pilot implementation, this would be a MARS Plugin for SODA. It would be aware of the MARS vocabulary and would be used by the \phname to communicate with MARS.
\end{enumerate}
\section{Limitations}
The data discovery process in ESGF is based on data which is already present and published; this means that the dynamic nature of MARS to generate customized data units cannot be exploited. What is possible is publishing a few popular combinations of requirements.
\section{Breakup of tasks}
\begin{enumerate}
\item Create an alternate publisher mechanism, to be able to publish offline data in a manner consistent with other ESGF publications, without the data being present on the disks. Metadata entries into the database etc must be consistent with normal ESGF publications.
\item Implement GRIB to NETCDF conversion functionality into the MARS plugin.
\item Implement ESGF to MARS query conversion in the MARS plugin
\item Finalize MARS data unit to ESGF facet mapping; what aspects of data are to be available.
\item Finalize what data needs to be made available
\item Ensure that the ESG-SEARCH and ESGF Web-Fronted are aware of th data's offline status and possible use this to provide a `Show offline datasets' option to users.
\item ESGF-SODA integration to service requests for these offline datasets.
\item Prior to publication of EURO4M data on ESGF, there is a need to define the DRS and data-attribute to ESGF Search facet mapping. This document needs to serve the
role of Martin Juckes' `CORDEX: ESGF Search Facet Mappings' document.
\end{enumerate}

\section{System Architecture}
Diagram of the system to go here...\\
\subsection{Components}
\begin{enumerate}
\item SODA WPS: interacted with by ESGF WebFE/CoG and SODA-generated wget script.
\item Backend Plugin aka MARS plugin
\item SODA Publisher: script to obtain data questionnaire from SODA WPS and perform insertion into ESGF database and other tasks to complete `ESGF publication'.
\item SODA Scheduler: to schedule publication requests and downloads requests.
\item SODA Messenger: to email notifications
\item SODA file server: for online hosting of requested files.
\end{enumerate}

\subsection{Use Case Diagram}
Use case diagram goes here...

\section{Issues and questions}
This is to be a place to list out problems that need to be handled.
\begin{enumerate}
\item Authorization in addition to authentication. Should we run a separate attribute service, like we do for CORDEX?
\end{enumerate}
\subsection{Web-Fe/CoG integration notes}
\begin{enumerate}
\item There are two possible approaches, for the manner in which the user's experience changes, based on whether the queried data exists in the online disk cache ornot.
\begin{enumerate}
\item The Web-Fe/CoG interacts with \phname for the requested data. If available online, it produces the wget script exactly as it would for normal online data.
\item If the data is not available on the disk cache, SODA’s response with a `please wait and retry' message is echoed to the user.
\end{enumerate}
\item Mixing of online and offline search results in the datacart can lead to complications, so it should simply not be possible to mix the two types, in one data cart session.
\end{enumerate}
\subsection{\phname notes}
\begin{enumerate}
\item \phname to be responsible for expiring staged data from disk; LRU?
\item \phname to maintain status of data; STAGED, OFFLINE, STAGING.
\item \phname to support manual purging of cached data.
\item Since it wouldn't be possible to generate checksums for MARS data which hasn't been
brought onto disk, the checksum would have to be left out of the publication phase.
The wget script should be obtained from the ESGF subsystem and then modiﬁed by \phname, before it is delivered to the user.
\item Authenticated user v/s unauthenticated user: what operations need to be restricted to authenticated users? For example, staging requests should only be possible for authenticated users while status query could be allowed for all.
\end{enumerate}

\subsection{MARS Plugin notes}
\begin{enumerate}
\item The ESGF search facet to MARS search facet/query conversion to be handled by the MARS plugin.
\item MARS plugin to be responsible for GRIB-NETCDF conversion.
\item This should be responsible for filling out the data questionnaire.
\item Data questionnaire should support both of the following cases:
\begin{enumerate}
\item Querying the backend for the required values and constructing the filled in questionairre.
\item Constructing the questionnaire with values provided by user.
\end{enumerate}
\item The plugin would be aware of the following:
\begin{enumerate}
\item Project specific DRS.
\item Project specific custom key/value pairs, for instance attributes which are common to all datasets belonging to a project which is NOT available from the DRS.
\end{enumerate}
\end{enumerate}

\section{Data download workflow}
\subsection{User}
\begin{enumerate}
\item User searches for using ESGF Search from WebFE/CoG and adds to datacart.
\item WebFE/CoG requests data retrieval from SODA WPS and prints out notification informing user that they'll get an email when the data is ready for download. \item The WebFE also provides user a SODA service URL (SURL) that he can use to get the status or download final SODA-generated wget script, when data is ready.
\item User gets email notification and SODA-generated wget script when data is made available online.
\item Script performs ESGF-style authentication and user executes script and gets data.
\end{enumerate}
\subsection{System}
\begin{enumerate}
\item WebFE/CoG interacts with SODA with list of datasets to be retrieved.
\item SODA to only allow authorized requests for data retrieval from the WebFE/CoG itself.
\item SODA submits request to scheduler and returns SODA service URL (SURL) to WebFE  
\item Scheduler looks up availability status prior to scheduling fetch operation
\item Scheduler to handle overlapping requests.
\end{enumerate}
\subsection{wget script and download tracking}
The system should track user downloads. This is to ensure that files which have been fetched completely by the user can be opportunistically popped out of the cache, in the event of high demand. The script should use checksums provided by the server to check in the event of a subsequent execution, to only fetch what is required and not attempt fetches of files already downloaded completely.
\section{Data publication workflow}
\begin{enumerate}
\item Prior to publication of offline data, a limiting constraint that would define the boundaries for the publication is specified. 
\item The limiting constraint could be a set of search facet like constraints; project=foo \& model=bar \& start\_date=xxx \& end\_date=yyy etc.
\item When offline data is to be published, a data questionnaire needs to be filled out. This questionnaire would contain all of the information required to be inserted into the postgresql database etc, that the standard ESGF publication involves. 
\item The plugin would perform sanity checks on the constraints specified by the user against the DRS and invalidate incorrect requests.
\item The limiting constraints would be used by the plugin, if it needs to query the backend for the required metadata to complete the questionnaire.
\item The plugin would also implement its own publisher, based on the ESGF publisher, which uses the data from the questionnaire to perform an ESGF publication.
\item Since the publication is done when the data isn't physically on online storage, the checksum information and size wouldn't be entered into the database. Where these are mandatory, placeholder values will be entered.
\item When a subsequent download request is processed, this information is `lazily' computed and the database updated. 
\end{enumerate}
\appendix
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
\setcounter{secnumdepth}{0}
\chapter{Jargon and abbrievations}
\begin{enumerate}
\item CLIPC: Climate Information Platform for Copernicus, EU project.
\item Web-FE : ESGF Web Frontend, provided by the index nodes in the federation.
\item \phname: System for Offline Data Access, developed for ESGF-CLIPC integration.
\item MARS Plugin: Bridge code to interact with \phname and the MARS system.
\end{enumerate}
\hypertarget{mymarker}{}
\printindex
\end{document}
